\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}

\title{Wave-Density Attention: Emergent Sharp Structure from Smooth Wave Interference}
\author{Conrad Kramer}
\date{}

\begin{document}
\maketitle

\begin{abstract}
Wave-based neural representations are appealing due to their parameter efficiency and ability to encode global structure, yet prior work has consistently struggled to represent sharp, localized features using smooth sinusoidal components. This limitation is often attributed to the inherent smoothness of wave functions, which makes explicit edge representation difficult without unstable high-frequency harmonics.

In this work, we propose \emph{Wave-Density Attention} (WDA), a novel attention mechanism that reframes wave computation through interference and cancellation, inspired by multi-pattern semiconductor lithography. Rather than interpreting wave amplitude as the represented quantity, WDA converts interference patterns into a density field, where sharp structure emerges statistically from constructive overlap and destructive nullification. Attention kernels are generated as sparse mixtures of wave masks, selected via a Mixture-of-Masks controller and applied efficiently using a causal Toeplitz convolution.

This formulation avoids explicit dot-product similarity and does not require sharp edges in the underlying wave functions. Despite wave parameters comprising less than $0.01\%$ of total model parameters, the resulting attention patterns are sparse, localized, and content-adaptive. We evaluate WDA using instruction-following language modeling as a real-world benchmark for sharp discrete selection. A 130M-parameter model achieves sub-20 perplexity on UltraChat, demonstrating stable training and coherent generation without dot-product attention.

Our results show that sharp, information-dense structure can emerge from smooth wave components when interference is treated as a computational primitive, suggesting a new direction for wave-based neural architectures.
\end{abstract}

\section{Introduction}
Self-attention based on dot-product similarity has become a foundational component of modern language models. Despite its effectiveness, dot-product attention imposes several constraints: quadratic memory cost, reliance on learned similarity spaces, and limited inductive bias toward structured or oscillatory interactions.

Inspired by physical interference phenomena, we explore an alternative formulation of attention in which interactions are mediated by wave superposition rather than vector similarity. In this framework, attention patterns emerge from the constructive and destructive interference of learnable wave functions over relative positions, producing sparse, structured attention densities.

This work introduces \emph{Wave-Density Attention}, a causal attention mechanism that:
\begin{itemize}
  \item Generates attention kernels via mixtures of wave masks (Mixture-of-Masks; MoM),
  \item Uses density normalization instead of softmax over dot products,
  \item Exploits Toeplitz structure for efficient causal computation,
  \item Separates attention pattern capacity from parameter count.
\end{itemize}

We show that this approach scales to real-world instruction-following language modeling, and we position language modeling as a demanding benchmark for sharp discrete selection.

\section{Related Work}
Wave-Density Attention intersects several research directions:
\begin{itemize}
  \item \textbf{Efficient attention:} Linear attention, kernelized attention, and low-rank methods aim to reduce quadratic cost. Our approach avoids dot products entirely and uses convolutional structure for efficiency.
  \item \textbf{Structured attention:} Relative-position biases, rotary embeddings, and convolutional hybrids introduce inductive structure. Wave interference provides an alternative structured prior.
  \item \textbf{State space models and convolutions:} Like SSMs, our model uses convolutional structure, but kernels are dynamically generated via learned wave mixtures rather than fixed recurrence.
  \item \textbf{Mixture-of-experts:} Sparse mask selection resembles MoE gating, though applied to attention kernels rather than feed-forward capacity.
\end{itemize}

\section{Wave-Density Attention with Mixture-of-Masks}
At a high level, instead of computing attention weights via pairwise query--key dot products, WDA constructs a causal attention kernel as a mixture of wave-generated masks, which is then applied to values via efficient convolution.

\subsection{Motivation}
Standard self-attention computes attention weights as:
\begin{equation}
\mathrm{Attn}(Q,K,V) = \mathrm{softmax}\left(\frac{QK^\top}{\sqrt{d}}\right)V.
\end{equation}

This formulation has several limitations:
\begin{itemize}
  \item Quadratic complexity in sequence length.
  \item Dependence on dense dot products, which are costly and sensitive to noise.
  \item Attention weights are directly parameterized by token embeddings, rather than emergent global structure.
\end{itemize}

We instead ask: can attention patterns be generated implicitly via structured interference, rather than explicitly computed similarity?

\subsection{Mixture-of-Masks (MoM)}
For each attention head, we maintain a small set of learnable wave masks $\{M_1,\dots,M_M\}$. A gating network selects a sparse subset of masks per input, producing a convex combination:
\begin{equation}
K(d) = \sum_{m=1}^{M} w_m\, M_m(d), \quad \sum_m w_m = 1,\; w_m \ge 0,
\end{equation}
where $d=i-j$ is relative token offset.

\subsection{Wave-Generated Masks}
Each mask is generated as a superposition of waves over relative positions:
\begin{equation}
M_m(d) = \sum_{w=1}^{W} a_{m,w}\, \phi\left(2\pi\langle f_{m,w}, p(d)\rangle + \theta_{m,w}\right),
\end{equation}
where $f_{m,w}$ (frequency), $a_{m,w}$ (amplitude), and $\theta_{m,w}$ (phase) are learnable, $p(d)$ is a positional encoding of offsets, and $\phi(\cdot)$ is a periodic function (triangle wave in our implementation).

\subsection{Density Transformation}
Raw wave superposition produces signed amplitudes. To obtain usable attention weights, we convert amplitudes into a density kernel:
\begin{equation}
D(d) = \sigma\big(\alpha\, K(d)\big),
\end{equation}
where $\sigma$ is a sigmoid and $\alpha$ controls sharpness.

\subsection{Causal Toeplitz Structure and Efficient Application}
Because attention depends only on relative position $d=i-j$, the resulting kernel is Toeplitz; causality restricts to $d\ge 0$. Applying attention reduces to a causal convolution:
\begin{equation}
\mathrm{Ctx}_i = \sum_{d=0}^{i} D(d)\, V_{i-d}.
\end{equation}

We implement this efficiently using FFT-based convolution in $\mathcal{O}(S\log S)$ time per head, avoiding explicit $S\times S$ attention matrices.

\subsection{Summary}
Wave-Density Attention replaces dot-product similarity with a structured, interference-based mechanism that is parameter-efficient, supports sparse and content-adaptive attention, scales efficiently with sequence length, and integrates into transformer-style architectures.

\subsection{Content-Conditioned Modulation}
To incorporate token content, we introduce lightweight modulation mechanisms:
\begin{itemize}
  \item \textbf{Global gating:} pooled token representations select active masks.
  \item \textbf{Key modulation:} per-token scalars modulate contribution to the convolution.
  \item \textbf{Low-rank mask scaling:} content-conditioned adjustments to mask weights.
\end{itemize}
These mechanisms allow attention patterns to adapt dynamically to input content, despite being generated from a small set of shared wave parameters.

\subsection{Comparison to Dot-Product Attention}
\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Dot-Product Attention} & \textbf{Wave-Density Attention} \\
\midrule
Core operation & $QK^\top$ & Wave interference \\
Attention weights & Explicit similarity & Emergent density \\
Complexity & $\mathcal{O}(S^2)$ & $\mathcal{O}(S\log S)$ (FFT) \\
Parameters & Dense projections & Sparse wave masks \\
Inductive bias & Local similarity & Global structure \\
\bottomrule
\end{tabular}
\end{center}
Importantly, Wave-Density Attention does not approximate dot-product attention. It represents a distinct computational primitive, with attention emerging from interference rather than similarity.

\section{Experiments}
We evaluate Wave-Density Attention (WDA) to test whether sharp, information-dense structure can emerge from smooth wave components via interference and cancellation, without explicitly encoding sharp edges in the underlying functions.

Rather than treating language modeling as the primary objective, we use it as a real-world benchmark for sharp discrete selection, where attention must localize, sparsify, and compose structure over long contexts.

Our experiments address the following questions:
\begin{itemize}
  \item Does interference-based density formation produce sparse and localized attention kernels?
  \item Can a small set of wave masks be composed via routing to express diverse patterns?
  \item Does this mechanism scale stably to large language modeling tasks?
\end{itemize}

\subsection{Emergent Sparsity from Wave Interference}
A core hypothesis of WDA is that sharp structure need not be explicitly represented in wave amplitude. Instead, sharpness can emerge statistically through constructive interference and destructive cancellation.

We observe that although individual wave masks are smooth, their superposition---followed by a density transformation---produces highly localized and sparse kernels. These kernels exhibit clear regions of activation separated by near-zero regions, despite the absence of discontinuities in the underlying wave functions.

\subsection{Density-Based Kernels versus Amplitude-Based Attention}
Using density (sigmoid-transformed interference, and in early experiments stochastic thresholding) improves training stability, sparsity of kernels, and downstream language modeling performance compared to using raw amplitudes.

\subsection{Compositional Routing via Mixture-of-Masks}
WDA employs a Mixture-of-Masks controller that sparsely selects a subset of wave masks per input. Increasing the number of masks and waves improves expressivity with diminishing returns; top-$k$ routing provides most of the gains.

\subsection{Language Modeling as Sharp Discrete Selection}
Language modeling requires selecting discrete tokens from a large vocabulary. We evaluate WDA on instruction-following language modeling as a proxy for real-world sharp selection.

\paragraph{UltraChat results.}
Best checkpoint perplexity: $17.5$. Mean evaluation (30 batches): loss $=3.0152$, perplexity $=20.39$.

\subsection{Training Dynamics and Stability}
Training shows smooth convergence with transient variance corresponding to data mixture transitions. No attention collapse or degenerate behavior is observed. Warmup plus cosine decay improves late-stage convergence.

\subsection{Efficiency and Scalability}
WDA avoids explicit $S\times S$ attention matrices. Toeplitz structure enables causal convolution via FFT, with linear memory in sequence length.

\section{Training Setup}
\subsection{Data}
We train from scratch using:
\begin{itemize}
  \item C4 (Common Crawl) for general language modeling.
  \item UltraChat for instruction-following and conversational coherence.
\end{itemize}

\subsection{Optimization}
\begin{itemize}
  \item Optimizer: AdamW.
  \item Learning rate: linear warmup plus cosine decay.
  \item Mixed precision: bfloat16.
  \item Gradient clipping for stability.
\end{itemize}

\section{Results}
\subsection{Language Modeling Performance}
\begin{itemize}
  \item Perplexity (C4): $290.42$ (loss $5.6713$; bits/byte $8.1820$).
  \item Perplexity (UltraChat): $26.04$ (loss $3.2596$).
  \item UltraChat mean evaluation (30 batches): loss $3.0152$, perplexity $20.39$.
\end{itemize}

\subsection{Qualitative Evaluation}
Generated samples demonstrate long-range coherence, instruction following, and thematic consistency. Despite lacking dot-product attention, the model exhibits behaviors typically associated with transformer decoders.

\section{Limitations and Future Work}
\begin{itemize}
  \item Performance on pure C4 lags after instruction-focused training; future work will explore stable multi-domain mixing to reduce forgetting.
  \item Further speed optimizations are possible via kernel caching and custom CUDA implementations.
  \item Scaling beyond 100M parameters remains future work.
\end{itemize}

\section{Conclusion}
We present Wave-Density Attention, a new attention mechanism based on wave interference rather than dot-product similarity. By converting interference into density and exploiting cancellation, WDA yields sharp, sparse attention structure without requiring sharp edges in the underlying wave functions. Our results demonstrate that coherent instruction-following language models can be trained from scratch using this approach.

\appendix

\section{Efficient Causal Application of Wave-Density Kernels}
\subsection{Toeplitz Structure of Relative Attention Kernels}
Wave-Density Attention generates attention weights as a function of relative token distance:
\begin{equation}
D(i,j) = D(i-j).
\end{equation}
This implies the attention matrix is Toeplitz (constant along diagonals). Causality restricts $i\ge j$, yielding a lower-triangular Toeplitz matrix specified by:
\begin{equation}
k = \{D(0), D(1), \dots, D(S-1)\}.
\end{equation}

\subsection{Attention as Causal Convolution}
Given values $V$, attention reduces to:
\begin{equation}
\mathrm{Ctx}_i = \sum_{j=0}^{i} D(i-j)\, V_j,
\end{equation}
which is causal 1D convolution with kernel $k$.

\subsection{FFT-Based Computation}
Using the convolution theorem,
\begin{equation}
\mathcal{F}(x * k) = \mathcal{F}(x) \odot \mathcal{F}(k),
\end{equation}
we compute causal convolution by zero-padding to length $2S$, FFT, elementwise multiply, inverse FFT, and truncation. This yields $\mathcal{O}(S\log S)$ time without materializing an $S\times S$ matrix.

\subsection{Normalization Without Softmax}
Normalization is computed by applying the same convolution to a scalar key-modulation signal $k_i$ and dividing:
\begin{equation}
\widehat{\mathrm{Ctx}}_i = \frac{\mathrm{Ctx}_i}{\mathrm{Norm}_i + \epsilon}.
\end{equation}

\subsection{Numerical Stability}
FFT is performed in float32 for stability; outputs are cast back to bfloat16/float16.

\subsection{Summary}
Toeplitz structure enables exact causal attention with linear memory and efficient FFT-based computation.

\section{Ablation Studies}
\subsection{Effect of Number of Masks (Mixture Capacity)}
\begin{itemize}
  \item masks=8, waves small: \mbox{$\sim 24$--$26$} UltraChat PPL (coarser patterns).
  \item masks=16: \mbox{$\sim 20$--$21$} UltraChat PPL (best trade-off).
  \item masks=32: \mbox{$\sim 20$} UltraChat PPL (diminishing returns).
\end{itemize}

\subsection{Waves per Mask (Interference Resolution)}
\begin{itemize}
  \item waves=2: \mbox{$\sim 26$--$28$} PPL.
  \item waves=4: \mbox{$\sim 22$--$24$} PPL.
  \item waves=8: \mbox{$\sim 20$--$21$} PPL.
  \item waves=16: \mbox{$\sim 20$} PPL (no significant improvement).
\end{itemize}

\subsection{Density Transformation vs Raw Amplitude}
\begin{itemize}
  \item Raw amplitude: unstable / diffuse attention.
  \item Sigmoid density: stable, \mbox{$\sim 20$} PPL.
  \item Stochastic threshold (early): stable, \mbox{$\sim 20$--$22$} PPL.
\end{itemize}

\subsection{Top-$k$ Routing}
\begin{itemize}
  \item topk=4: \mbox{$\sim 23$--$25$} PPL.
  \item topk=8: \mbox{$\sim 20$--$21$} PPL.
  \item topk=16: \mbox{$\sim 20$} PPL.
\end{itemize}

\subsection{FFN Width}
Increasing FFN width increases compute with marginal gains; most gains arise from attention structure.

\section{Figure Captions}
\paragraph{Figure 1 --- Wave Interference to Density Transformation.}
Smooth wave components combine via constructive and destructive interference. Sharp, localized density peaks emerge after thresholding or sigmoid transformation, despite the absence of discontinuities in the underlying waves.

\paragraph{Figure 2 --- Mixture-of-Masks Routing.}
A sparse gating network selects a subset of wave masks per input. Selected masks are superposed to form an attention kernel, enabling compositional expressivity with minimal parameter overhead.

\paragraph{Figure 3 --- Attention Kernel Sparsity.}
Example kernels generated by WDA, showing localized receptive fields and sharp boundaries emerging from smooth wave interference.

\paragraph{Figure 4 --- Training Dynamics.}
Validation perplexity versus tokens processed, showing stable convergence and recovery from transient spikes.

\end{document}
