{
  "teacher_model_id": "HuggingFaceTB/SmolLM-135M",
  "datasets": [
    {
      "kind": "hf_text",
      "dataset_name": "HuggingFaceTB/smollm-corpus",
      "subset": "cosmopedia-v2",
      "split": "train",
      "text_column": "text",
      "ratio": 0.4
    },
    {
      "kind": "hf_text",
      "dataset_name": "HuggingFaceTB/smollm-corpus",
      "subset": "fineweb-edu-dedup",
      "split": "train",
      "text_column": "text",
      "ratio": 0.4
    },
    {
      "kind": "hf_text",
      "dataset_name": "Avelina/python-edu",
      "split": "train",
      "text_column": "text",
      "ratio": 0.2
    }
  ],
  "seq_len": 128,
  "micro_batch_size": 32,
  "grad_accum": 4,

  "steps": 6600,
  "lr": 0.0003,
  "weight_decay": 0.01,
  "warmup_steps": 50,
  "max_grad_norm": 1.0,

  "temperature": 1.1,
  "kl_weight": 1.0,
  "ce_weight": 0.5,
  "hidden_match_weight": 0.0,

  "handoff_power": 1.0,
  "alpha_max": 1.0,
  "alpha_warmup_steps": 2000,
  "teacher_scale_start": 1.0,
  "teacher_scale_end": 0.0,
  "wda_scale_start": 1.0,
  "wda_scale_end": 1.0,
  "handoff_start_step": 200,
  "handoff_mode": "global",
  "handoff_steps": 2500,
  "handoff_stabilization_steps": 500,

  "freeze_converted": false,
  "converted_lr_scale": 1.0,
  "init_gamma": 0.0,

  "autopilot_enabled": true,
  "autopilot_kl_low": 0.2,
  "autopilot_kl_high": 0.8,

  "ema_teacher_enabled": true,
  "ema_decay": 0.9995,
  "ema_start_progress": 5600,

  "ema_alpha_cap": 0.995,
  "ema_alpha_cap_end": 1.0,
  "ema_alpha_cap_steps": 600,

  "ema_teacher_scale_floor": 0.07,
  "ema_teacher_scale_floor_end": 0.01,
  "ema_teacher_scale_floor_steps": 900,

  "ema_kl_weight": 0.2,
  "ema_ce_weight": 1.5,
  "ema_gate_temp_start": 0.08,
  "ema_gate_temp_end": 0.05,
  "ema_gate_temp_steps": 900,

  "out_dir": "private/checkpoints/smol135m_wda",
  "device": "cuda",
  "torch_dtype": "bfloat16",

  "save_every": 10,
  "log_every": 1,

  "final_force_teacher_off": false,

  "wda_num_masks": 16,
  "wda_num_waves_per_mask": 8,
  "wda_topk_masks": 4,
  "wda_gate_temp": 0.05,
  "wda_attn_alpha": 3.0,

  "gamma_l1_weight": 0.01,
  "gamma_l1_target": 0.5,
  "routing_entropy_weight": 0.02
}
