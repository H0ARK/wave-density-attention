{
    "teacher_model_id": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
    "datasets": [
        {
            "kind": "hf_chat",
            "dataset_name": "nvidia/Llama-Nemotron-Post-Training-Dataset",
            "split": "chat",
            "ratio": 0.90,
            "assistant_only_loss": true
        },
        {
            "kind": "hf_chat",
            "dataset_name": "nvidia/Llama-Nemotron-Post-Training-Dataset",
            "split": "math",
            "ratio": 0.020,
            "assistant_only_loss": true
        },
        {
            "kind": "hf_chat",
            "dataset_name": "nvidia/Llama-Nemotron-Post-Training-Dataset",
            "split": "code",
            "ratio": 0.020,
            "assistant_only_loss": true
        },
        {
            "kind": "hf_text",
            "dataset_name": "HuggingFaceFW/fineweb-edu",
            "subset": "sample-10BT",
            "split": "train",
            "text_column": "text",
            "ratio": 0.020
        },
        {
            "kind": "hf_chat",
            "dataset_name": "HuggingFaceH4/ultrafeedback_binarized",
            "split": "train_sft",
            "assistant_only_loss": true,
            "ratio": 0.020
        },
        {
            "kind": "local_jsonl",
            "path": "private/data/tech_cache/chat_if.jsonl",
            "is_chat": true,
            "assistant_only_loss": true,
            "ratio": 0.010
        },
        {
            "kind": "local_jsonl",
            "path": "private/data/tech_cache/Nemotron-Pretraining-Scientific-Coding.jsonl",
            "ratio": 0.010
        }
    ],
    "seq_len": 128,
    "micro_batch_size": 24,
    "grad_accum": 16,
    "steps": 10000,
    "lr": 0.0004,
    "weight_decay": 0.05,
    "warmup_steps": 500,
    "max_grad_norm": 1.0,
    "out_dir": "private/checkpoints/scratch_nemotron_wda",
    "device": "cuda",
    "torch_dtype": "bfloat16",
    "use_compile": true,
    "compile_mode": "reduce-overhead",
    "save_every": 500,
    "generate_every": 50,
    "log_every": 1,
    "use_chunks": false,
    "chunk_size": 64,
    "chunk_reg_weight": 0.01,
    "embed_dim": 1024,
    "num_layers": 8,
    "num_heads": 6,
    "wda_num_masks": 128,
    "wda_num_waves_per_mask": 4,
    "wda_topk_masks": 8,
    "wda_attn_alpha": 3.0
}
